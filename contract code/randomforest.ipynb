{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffb9e38e-c857-4a12-99ec-287bcc0f0be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import logging\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from PIL import Image\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import word2vec\n",
    "from sklearn.cluster import KMeans \n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.corpus import wordnet\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9518df09-c90a-46d1-b5e8-2cd9a2d240d4",
   "metadata": {},
   "source": [
    "## Load Data and Data Proprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c500eafb-1c6a-4d5c-8e38-c17ab27955f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train = pd.read_csv(\"../data/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv(\"../data/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "df_train1=pd.read_csv(\"../data/imdb_master.csv\",encoding=\"latin-1\")\n",
    "\n",
    "#To ensure sufficient data set, mix all 50,000 pieces of data in label and test set.\n",
    "\n",
    "test.insert(1, 'sentiment', 0)\n",
    "for i in range(len(test)):\n",
    "    if int(re.sub(\"[^0-9]\", \" \", test[\"id\"][i].split(\"_\")[1])) <= 5:\n",
    "        test['sentiment']=0\n",
    "    elif int(re.sub(\"[^0-9]\", \" \", test[\"id\"][i].split(\"_\")[1])) > 5:\n",
    "        test['sentiment']=1\n",
    "\n",
    "# Divided into train set and test set according to 8: 2\n",
    "test_set,train_set=train_test_split(test,test_size=0.6,random_state=1)\n",
    "train_set=train.append(train_set)\n",
    "                                 \n",
    "\n",
    "#Processing the imported external data, eliminating irrelevant and unsupervised items and unifying the format.\n",
    "df_train1=df_train1.drop([\"type\",'file'],axis=1)\n",
    "df_train1.rename(columns={'label':'sentiment',\n",
    "                          'Unnamed: 0':'id',\n",
    "                          'review':'review'}, \n",
    "                 inplace=True)               \n",
    "df_train1 = df_train1[df_train1.sentiment != 'unsup']\n",
    "maping = {'pos': 1, 'neg': 0}\n",
    "df_train1['sentiment'] = df_train1['sentiment'].map(maping)\n",
    "\n",
    "train_set=train_set.reset_index(drop=True)\n",
    "test_set=test_set.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03864b5e-2c98-4379-a037-fb74a04af1fc",
   "metadata": {},
   "source": [
    "## Train the Word2vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "071d19a3-a1d6-4845-8de0-f2a2efc565d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 22:27:09,584 : INFO : collecting all words and their counts\n",
      "2022-04-21 22:27:09,587 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2022-04-21 22:27:09,617 : INFO : PROGRESS: at sentence #10000, processed 225908 words, keeping 17776 word types\n",
      "2022-04-21 22:27:09,647 : INFO : PROGRESS: at sentence #20000, processed 452097 words, keeping 24953 word types\n",
      "2022-04-21 22:27:09,679 : INFO : PROGRESS: at sentence #30000, processed 671984 words, keeping 30044 word types\n",
      "2022-04-21 22:27:09,712 : INFO : PROGRESS: at sentence #40000, processed 898757 words, keeping 34358 word types\n",
      "2022-04-21 22:27:09,747 : INFO : PROGRESS: at sentence #50000, processed 1122100 words, keeping 37824 word types\n",
      "2022-04-21 22:27:09,781 : INFO : PROGRESS: at sentence #60000, processed 1341946 words, keeping 40782 word types\n",
      "2022-04-21 22:27:09,820 : INFO : PROGRESS: at sentence #70000, processed 1566792 words, keeping 43383 word types\n",
      "2022-04-21 22:27:09,854 : INFO : PROGRESS: at sentence #80000, processed 1785875 words, keeping 45762 word types\n",
      "2022-04-21 22:27:09,889 : INFO : PROGRESS: at sentence #90000, processed 2010133 words, keeping 48195 word types\n",
      "2022-04-21 22:27:09,923 : INFO : PROGRESS: at sentence #100000, processed 2231528 words, keeping 50243 word types\n",
      "2022-04-21 22:27:09,957 : INFO : PROGRESS: at sentence #110000, processed 2451526 words, keeping 52128 word types\n",
      "2022-04-21 22:27:09,993 : INFO : PROGRESS: at sentence #120000, processed 2674779 words, keeping 54171 word types\n",
      "2022-04-21 22:27:10,027 : INFO : PROGRESS: at sentence #130000, processed 2899961 words, keeping 55883 word types\n",
      "2022-04-21 22:27:10,061 : INFO : PROGRESS: at sentence #140000, processed 3117845 words, keeping 57440 word types\n",
      "2022-04-21 22:27:10,098 : INFO : PROGRESS: at sentence #150000, processed 3343821 words, keeping 59128 word types\n",
      "2022-04-21 22:27:10,132 : INFO : PROGRESS: at sentence #160000, processed 3566493 words, keeping 60670 word types\n",
      "2022-04-21 22:27:10,166 : INFO : PROGRESS: at sentence #170000, processed 3789789 words, keeping 62146 word types\n",
      "2022-04-21 22:27:10,200 : INFO : PROGRESS: at sentence #180000, processed 4012438 words, keeping 63562 word types\n",
      "2022-04-21 22:27:10,235 : INFO : PROGRESS: at sentence #190000, processed 4238418 words, keeping 64874 word types\n",
      "2022-04-21 22:27:10,271 : INFO : PROGRESS: at sentence #200000, processed 4461933 words, keeping 66163 word types\n",
      "2022-04-21 22:27:10,305 : INFO : PROGRESS: at sentence #210000, processed 4685063 words, keeping 67472 word types\n",
      "2022-04-21 22:27:10,339 : INFO : PROGRESS: at sentence #220000, processed 4910486 words, keeping 68775 word types\n",
      "2022-04-21 22:27:10,375 : INFO : PROGRESS: at sentence #230000, processed 5132364 words, keeping 70035 word types\n",
      "2022-04-21 22:27:10,411 : INFO : PROGRESS: at sentence #240000, processed 5358800 words, keeping 71249 word types\n",
      "2022-04-21 22:27:10,446 : INFO : PROGRESS: at sentence #250000, processed 5571813 words, keeping 72424 word types\n",
      "2022-04-21 22:27:10,480 : INFO : PROGRESS: at sentence #260000, processed 5794437 words, keeping 73561 word types\n",
      "2022-04-21 22:27:10,515 : INFO : PROGRESS: at sentence #270000, processed 6011683 words, keeping 74808 word types\n",
      "2022-04-21 22:27:10,557 : INFO : PROGRESS: at sentence #280000, processed 6234714 words, keeping 76425 word types\n",
      "2022-04-21 22:27:10,597 : INFO : PROGRESS: at sentence #290000, processed 6455421 words, keeping 77811 word types\n",
      "2022-04-21 22:27:10,635 : INFO : PROGRESS: at sentence #300000, processed 6677515 words, keeping 79085 word types\n",
      "2022-04-21 22:27:10,670 : INFO : PROGRESS: at sentence #310000, processed 6897567 words, keeping 80377 word types\n",
      "2022-04-21 22:27:10,704 : INFO : PROGRESS: at sentence #320000, processed 7115954 words, keeping 81538 word types\n",
      "2022-04-21 22:27:10,740 : INFO : PROGRESS: at sentence #330000, processed 7338668 words, keeping 82726 word types\n",
      "2022-04-21 22:27:10,775 : INFO : PROGRESS: at sentence #340000, processed 7558900 words, keeping 83862 word types\n",
      "2022-04-21 22:27:10,809 : INFO : PROGRESS: at sentence #350000, processed 7780626 words, keeping 85037 word types\n",
      "2022-04-21 22:27:10,846 : INFO : PROGRESS: at sentence #360000, processed 8007912 words, keeping 86137 word types\n",
      "2022-04-21 22:27:10,880 : INFO : PROGRESS: at sentence #370000, processed 8229815 words, keeping 87243 word types\n",
      "2022-04-21 22:27:10,922 : INFO : PROGRESS: at sentence #380000, processed 8453334 words, keeping 88355 word types\n",
      "2022-04-21 22:27:10,958 : INFO : PROGRESS: at sentence #390000, processed 8674826 words, keeping 89355 word types\n",
      "2022-04-21 22:27:10,996 : INFO : PROGRESS: at sentence #400000, processed 8891945 words, keeping 90314 word types\n",
      "2022-04-21 22:27:11,037 : INFO : PROGRESS: at sentence #410000, processed 9113088 words, keeping 91246 word types\n",
      "2022-04-21 22:27:11,078 : INFO : PROGRESS: at sentence #420000, processed 9338016 words, keeping 92218 word types\n",
      "2022-04-21 22:27:11,089 : INFO : collected 92460 word types from a corpus of 9395270 raw words and 422542 sentences\n",
      "2022-04-21 22:27:11,089 : INFO : Creating a fresh vocabulary\n",
      "2022-04-21 22:27:11,157 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=40 retains 11238 unique words (12.154445165476963%% of original 92460, drops 81222)', 'datetime': '2022-04-21T22:27:11.155979', 'gensim': '4.1.2', 'python': '3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2022-04-21 22:27:11,157 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=40 leaves 8958626 word corpus (95.35251248766667%% of original 9395270, drops 436644)', 'datetime': '2022-04-21T22:27:11.157067', 'gensim': '4.1.2', 'python': '3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2022-04-21 22:27:11,210 : INFO : deleting the raw counts dictionary of 92460 items\n",
      "2022-04-21 22:27:11,212 : INFO : sample=0.001 downsamples 49 most-common words\n",
      "2022-04-21 22:27:11,213 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 6567535.008588498 word corpus (73.3%% of prior 8958626)', 'datetime': '2022-04-21T22:27:11.213490', 'gensim': '4.1.2', 'python': '3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2022-04-21 22:27:11,294 : INFO : estimated required memory for 11238 words and 300 dimensions: 32590200 bytes\n",
      "2022-04-21 22:27:11,295 : INFO : resetting layer weights\n",
      "2022-04-21 22:27:11,305 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-04-21T22:27:11.305739', 'gensim': '4.1.2', 'python': '3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'build_vocab'}\n",
      "2022-04-21 22:27:11,306 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 11238 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2022-04-21T22:27:11.306739', 'gensim': '4.1.2', 'python': '3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n",
      "2022-04-21 22:27:12,314 : INFO : EPOCH 1 - PROGRESS: at 18.52% examples, 1217607 words/s, in_qsize 7, out_qsize 0\n",
      "2022-04-21 22:27:13,314 : INFO : EPOCH 1 - PROGRESS: at 36.79% examples, 1209487 words/s, in_qsize 7, out_qsize 0\n",
      "2022-04-21 22:27:14,323 : INFO : EPOCH 1 - PROGRESS: at 55.54% examples, 1217383 words/s, in_qsize 7, out_qsize 0\n",
      "2022-04-21 22:27:15,324 : INFO : EPOCH 1 - PROGRESS: at 75.18% examples, 1231189 words/s, in_qsize 7, out_qsize 0\n",
      "2022-04-21 22:27:16,332 : INFO : EPOCH 1 - PROGRESS: at 95.33% examples, 1246801 words/s, in_qsize 7, out_qsize 0\n",
      "2022-04-21 22:27:16,550 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-04-21 22:27:16,551 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-04-21 22:27:16,558 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-04-21 22:27:16,560 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-04-21 22:27:16,561 : INFO : EPOCH - 1 : training on 9395270 raw words (6569151 effective words) took 5.3s, 1251127 effective words/s\n",
      "2022-04-21 22:27:17,566 : INFO : EPOCH 2 - PROGRESS: at 20.71% examples, 1367511 words/s, in_qsize 7, out_qsize 0\n",
      "2022-04-21 22:27:18,573 : INFO : EPOCH 2 - PROGRESS: at 41.89% examples, 1374500 words/s, in_qsize 7, out_qsize 0\n",
      "2022-04-21 22:27:19,574 : INFO : EPOCH 2 - PROGRESS: at 63.13% examples, 1380966 words/s, in_qsize 7, out_qsize 0\n",
      "2022-04-21 22:27:20,577 : INFO : EPOCH 2 - PROGRESS: at 84.56% examples, 1384967 words/s, in_qsize 7, out_qsize 0\n",
      "2022-04-21 22:27:21,300 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-04-21 22:27:21,301 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-04-21 22:27:21,302 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-04-21 22:27:21,304 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-04-21 22:27:21,304 : INFO : EPOCH - 2 : training on 9395270 raw words (6567462 effective words) took 4.7s, 1385943 effective words/s\n",
      "2022-04-21 22:27:22,309 : INFO : EPOCH 3 - PROGRESS: at 20.21% examples, 1333608 words/s, in_qsize 7, out_qsize 0\n",
      "2022-04-21 22:27:23,311 : INFO : EPOCH 3 - PROGRESS: at 40.48% examples, 1333022 words/s, in_qsize 7, out_qsize 0\n",
      "2022-04-21 22:27:24,312 : INFO : EPOCH 3 - PROGRESS: at 61.15% examples, 1342156 words/s, in_qsize 7, out_qsize 0\n",
      "2022-04-21 22:27:25,311 : INFO : EPOCH 3 - PROGRESS: at 81.40% examples, 1335726 words/s, in_qsize 7, out_qsize 0\n",
      "2022-04-21 22:27:26,195 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-04-21 22:27:26,199 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-04-21 22:27:26,207 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-04-21 22:27:26,211 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-04-21 22:27:26,211 : INFO : EPOCH - 3 : training on 9395270 raw words (6568483 effective words) took 4.9s, 1339696 effective words/s\n",
      "2022-04-21 22:27:27,231 : INFO : EPOCH 4 - PROGRESS: at 21.15% examples, 1375502 words/s, in_qsize 7, out_qsize 0\n",
      "2022-04-21 22:27:28,238 : INFO : EPOCH 4 - PROGRESS: at 42.40% examples, 1382107 words/s, in_qsize 7, out_qsize 0\n",
      "2022-04-21 22:27:29,241 : INFO : EPOCH 4 - PROGRESS: at 63.77% examples, 1387642 words/s, in_qsize 7, out_qsize 0\n",
      "2022-04-21 22:27:30,248 : INFO : EPOCH 4 - PROGRESS: at 85.17% examples, 1388114 words/s, in_qsize 7, out_qsize 0\n",
      "2022-04-21 22:27:30,922 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-04-21 22:27:30,928 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-04-21 22:27:30,930 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-04-21 22:27:30,932 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-04-21 22:27:30,932 : INFO : EPOCH - 4 : training on 9395270 raw words (6567904 effective words) took 4.7s, 1392763 effective words/s\n",
      "2022-04-21 22:27:31,937 : INFO : EPOCH 5 - PROGRESS: at 19.56% examples, 1290166 words/s, in_qsize 7, out_qsize 0\n",
      "2022-04-21 22:27:32,945 : INFO : EPOCH 5 - PROGRESS: at 39.86% examples, 1307130 words/s, in_qsize 7, out_qsize 0\n",
      "2022-04-21 22:27:33,949 : INFO : EPOCH 5 - PROGRESS: at 60.72% examples, 1328187 words/s, in_qsize 7, out_qsize 0\n",
      "2022-04-21 22:27:34,966 : INFO : EPOCH 5 - PROGRESS: at 80.97% examples, 1319853 words/s, in_qsize 7, out_qsize 0\n",
      "2022-04-21 22:27:35,877 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-04-21 22:27:35,878 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-04-21 22:27:35,890 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-04-21 22:27:35,892 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-04-21 22:27:35,892 : INFO : EPOCH - 5 : training on 9395270 raw words (6567701 effective words) took 5.0s, 1325072 effective words/s\n",
      "2022-04-21 22:27:35,893 : INFO : Word2Vec lifecycle event {'msg': 'training on 46976350 raw words (32840701 effective words) took 24.6s, 1335715 effective words/s', 'datetime': '2022-04-21T22:27:35.893733', 'gensim': '4.1.2', 'python': '3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n",
      "2022-04-21 22:27:35,893 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec(vocab=11238, vector_size=300, alpha=0.025)', 'datetime': '2022-04-21T22:27:35.893733', 'gensim': '4.1.2', 'python': '3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n",
      "2022-04-21 22:27:35,898 : WARNING : destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n",
      "2022-04-21 22:27:35,900 : INFO : Word2Vec lifecycle event {'fname_or_handle': '300features_40minwords_10context', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-04-21T22:27:35.900734', 'gensim': '4.1.2', 'python': '3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'saving'}\n",
      "2022-04-21 22:27:35,901 : INFO : not storing attribute cum_table\n",
      "2022-04-21 22:27:35,938 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "#Data cleaning function\n",
    "def review_to_wordlist( review, remove_stopwords=False ):\n",
    "    \n",
    "    #Use the BeautifulSoup library to get rid of the original text and remove symbols such as < br/>.\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "\n",
    "    #Remove non-letters with regular expression\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "\n",
    "    #Convert to lower case, split into individual words\n",
    "    words = review_text.lower().split()\n",
    "\n",
    "    #Create set of stopwords and remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "        \n",
    "    #return the result.\n",
    "    return(words)\n",
    "\n",
    "#Load the punkt tokenizer and break the paragraph into different sentences, because Word2Vec needs a single sentence.\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "    \n",
    "    #Define a function to split a comment into parsed sentences.\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "\n",
    "    #Function to split a comment into parsed sentences.\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append( review_to_wordlist( raw_sentence, remove_stopwords ))\n",
    "\n",
    "    #Returns a list of lists\n",
    "    return sentences\n",
    "sentences = []  \n",
    "\n",
    "for review in train_set[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "    \n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)\n",
    "\n",
    "#Set values for various parameters\n",
    "num_features = 300                   \n",
    "min_word_count = 40                     \n",
    "num_workers = 4       \n",
    "context = 10                                                                                            \n",
    "downsampling = 1e-3   \n",
    "\n",
    "# Use Word2Vec algorithm\n",
    "from gensim.models import word2vec\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            vector_size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# save the model\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc89873d-ec2f-424c-9010-2183fb04c95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for K Means clustering:  330.8599421977997 seconds.\n"
     ]
    }
   ],
   "source": [
    "#Using K-means clustering method\n",
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "\n",
    "start = time.time() \n",
    "\n",
    "#Set k(num _ clusters) to 1/5 of the vocabulary, or an average of 5 words per cluster.\n",
    "word_vectors = model.wv.vectors\n",
    "num_clusters = int(word_vectors.shape[0] / 5)\n",
    "\n",
    "#Initialize a k-means object and use it to extract the centroid. The incoming K-means must be of type int.\n",
    "kmeans_clustering = KMeans( n_clusters = num_clusters )\n",
    "idx = kmeans_clustering.fit_predict( word_vectors )\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(\"Time taken for K Means clustering: \", elapsed, \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e60de5d2-e647-492f-8841-f65a05c3569b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0\n",
      "['controlled', 'motivated', 'consumed', 'insulted', 'blinded']\n",
      "\n",
      "Cluster 1\n",
      "['bush', 'advertising', 'counter', 'campaign', 'ch', 'coverage', 'administration', 'newspapers']\n",
      "\n",
      "Cluster 2\n",
      "['disguised', 'posing', 'glorified', 'monstrous']\n",
      "\n",
      "Cluster 3\n",
      "['monroe', 'marilyn', 'lana']\n",
      "\n",
      "Cluster 4\n",
      "['window', 'cutting', 'lights', 'doors', 'walls', 'dust', 'buildings', 'pan', 'rooms', 'trees', 'equipment', 'dirt', 'tube', 'windows', 'furniture', 'pans', 'lens', 'zoom', 'roads', 'boxes', 'corners', 'chairs', 'filter']\n",
      "\n",
      "Cluster 5\n",
      "['bloom', 'heath', 'liam', 'ledger', 'turturro', 'leonardo', 'hayden', 'cathy', 'dicaprio', 'orlando', 'neeson']\n",
      "\n",
      "Cluster 6\n",
      "['it', 'this']\n",
      "\n",
      "Cluster 7\n",
      "['self', 'indulgent', 'conscious', 'absorbed', 'righteous', 'proclaimed', 'indulgence', 'consciously']\n",
      "\n",
      "Cluster 8\n",
      "['pitt', 'sutherland', 'jamie', 'macy', 'jeffrey', 'bridges', 'nicholas', 'paxton', 'pierce', 'reliable', 'busey', 'pegg', 'arkin', 'dafoe', 'bon', 'dominic', 'wahlberg', 'bernsen', 'keitel', 'pertwee', 'sykes', 'kiefer', 'viggo', 'lamas', 'dourif', 'willem', 'mortensen']\n",
      "\n",
      "Cluster 9\n",
      "['ignore', 'overlook']\n"
     ]
    }
   ],
   "source": [
    "#Create a word/index dictionary and map each vocabulary word to a cluster number.             \n",
    "word_centroid_map = dict(zip( model.wv.index_to_key, idx ))\n",
    "for cluster in range(0,10):\n",
    "\n",
    "    print(\"\\nCluster %d\" % cluster)\n",
    "    words = []\n",
    "    for i in range(0,len(word_centroid_map.values())):\n",
    "        if(list(word_centroid_map.values())[i] == cluster):\n",
    "            words.append(list(word_centroid_map.keys())[i])\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47e1ac8e-17e8-49c9-af7b-2f67028c1aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_reviews = []\n",
    "for review in train_set[\"review\"]:\n",
    "    clean_train_reviews.append( review_to_wordlist( review, remove_stopwords=True ))\n",
    "\n",
    "clean_test_reviews = []\n",
    "for review in test_set[\"review\"]:\n",
    "    clean_test_reviews.append( review_to_wordlist( review, remove_stopwords=True ))\n",
    "\n",
    "#The centroid bag function converts comments into centroid bags    \n",
    "def create_bag_of_centroids( wordlist, word_centroid_map ):\n",
    "\n",
    "    #The number of clusters is equal to the highest cluster index in the word/centroid graph.\n",
    "    num_centroids = max( word_centroid_map.values() ) + 1\n",
    "\n",
    "    # Pre-allocate the bag of centroids vector (for speed)\n",
    "    bag_of_centroids = np.zeros( num_centroids, dtype=\"float32\" )\n",
    "\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "\n",
    "    # Return the \"bag of centroids\"\n",
    "    return bag_of_centroids\n",
    "\n",
    "#Assign an array (for speed) to the training set package of the centroid in advance.\n",
    "train_centroids = np.zeros( (train_set[\"review\"].size, num_clusters), \\\n",
    "    dtype=\"float32\" )\n",
    "\n",
    "# Transform the training set reviews into bags of centroids\n",
    "counter = 0\n",
    "for review in clean_train_reviews:\n",
    "    train_centroids[counter] = create_bag_of_centroids( review, \\\n",
    "        word_centroid_map )\n",
    "    counter += 1\n",
    "\n",
    "# Repeat for test reviews \n",
    "test_centroids = np.zeros(( test_set[\"review\"].size, num_clusters), \\\n",
    "    dtype=\"float32\" )\n",
    "\n",
    "counter = 0\n",
    "for review in clean_test_reviews:\n",
    "    test_centroids[counter] = create_bag_of_centroids( review, \\\n",
    "        word_centroid_map )\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bebaca-d19f-4a03-80a5-53b6137ed729",
   "metadata": {},
   "source": [
    "## Random Forest Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e075352d-c20e-436e-899c-3d2ba2fb3b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n"
     ]
    }
   ],
   "source": [
    "#Fitting random forest and extracting prediction\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "# Fitting the forest may take a few minutes\n",
    "print(\"Fitting a random forest to labeled training data...\")\n",
    "forest = forest.fit(train_centroids,train_set[\"sentiment\"])\n",
    "result = forest.predict(test_centroids)\n",
    "\n",
    "output = pd.DataFrame(data={\"id\":test_set[\"id\"], \"sentiment\":test_set[\"sentiment\"],\"sentiment_new\":result})\n",
    "output.to_csv( \"BagOfCentroids.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38ef73a2-e569-4fb8-b5c9-b55fa0335fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8799\n"
     ]
    }
   ],
   "source": [
    "#judge the result\n",
    "count=0\n",
    "model = pd.read_csv(\"BagOfCentroids.csv\");\n",
    "for i in range(model.shape[0]):\n",
    "    if model[\"sentiment\"][i]==model[\"sentiment_new\"][i]:\n",
    "        count=count+1\n",
    "        \n",
    "print(count/model.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
